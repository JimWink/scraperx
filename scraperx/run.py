import sys
import json
import logging
import argparse
from .utils import import_scraper
from .write_to import WriteTo

logger = logging.getLogger(__name__)


def validate_config(args):
    """Validate the scraper config file

    Arguments:
        args {argparse.Namespace} -- Cli args
    """
    logger.info(f"Validate the config file {args.config}")
    from .config import Config
    try:
        Config.get_config_from_file(args.config)
    except ValueError as e:
        logger.critical(e)
        # Should not continue if there is an issue with the config
        sys.exit(1)
    logger.info("Config file is valid")


def run_dispatch(args):
    """Kick off the dispatcher for the scraper

    Arguments:
        args {argparse.Namespace} -- Cli args
    """
    dispatcher = args.scraper.Dispatch(cli_args=args)

    if args.dump_tasks:
        # Dump data to local json file
        task_file = WriteTo(dispatcher.tasks).write_json()\
                                             .save_local('tasks.json')
        logger.info(f"Saved {len(dispatcher.tasks)} tasks to {task_file}")

    if args.dispatch:
        dispatcher.dispatch(standalone=args.standalone)


def run_downloader(args):
    """Kick off the downloader for the scraper

    Arguments:
        args {argparse.Namespace} -- Cli args
    """
    for task in args.tasks:
        downloader = args.scraper.Download(task, cli_args=args)
        downloader.run(save_metadata=args.save_metadata,
                       standalone=args.standalone)


def read_tasks(task_file):
    """Read in the tasks to a list

    Read in the json tasks file generated by the dispatcher
    and create a lists of tasks to be processed

    Arguments:
        task_file {str} -- Path the the task.json file

    Returns:
        list -- List of tasks to be processed
    """
    tasks = []
    with open(task_file) as f:
        tasks = json.load(f)

    if not isinstance(tasks, (list, tuple)):
        tasks = [tasks]

    return tasks


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run platform3')

    subparsers = parser.add_subparsers(dest='command', help='sub-command help')

    ###
    # Validation
    ###
    parser_validate = subparsers.add_parser('validate',
                                            help="Validate the configs")
    parser_validate.add_argument('config',
                                 help="The config.yaml to validate")

    ###
    # Dispatching
    ###
    parser_dispatch = subparsers.add_parser('dispatch',
                                            help="Run the dispatcher")
    parser_dispatch.add_argument('scraper', type=import_scraper,
                                 help="The scraper module to run")

    # At least one of these in a single group must be used
    # TODO: if dump-tasks, then the other 2 cannot be used
    parser_dispatch.add_argument('--dump-tasks', action='store_true',
                                 help="Save the tasks as json")
    # These can be used together though
    parser_dispatch.add_argument('--standalone', action='store_true',
                                 help="Trigger the downloader")
    parser_dispatch.add_argument('--dispatch', action='store_true',
                                 help="Loop through the tasks")

    ratelimit_group = parser_dispatch.add_mutually_exclusive_group()
    ratelimit_group.add_argument('--qps', type=float,
                                 help='Number of tasks to dispatch a second')
    ratelimit_group.add_argument('--period', type=float,
                                 help='Dispatch all tasks in n hours')

    parser_dispatch.add_argument('--limit', type=int,
                                 help="Limit the number of tasks")

    ###
    # Downloading
    ###
    parser_download = subparsers.add_parser('download',
                                            help="Run the downloader")
    parser_download.add_argument('scraper', type=import_scraper,
                                 help="The scraper module to run")
    parser_download.add_argument('tasks', type=read_tasks,
                                 help="Task file of things to run")
    parser_download.add_argument('--standalone', action='store_true',
                                 help="Trigger the extractor")
    parser_download.add_argument('--save-metadata', action='store_true',
                                 help="Save a metadata file with the source")

    ###
    # Extracting
    ###
    parser_extract = subparsers.add_parser('extract',
                                           help="Run the extractor")
    parser_extract.add_argument('scraper', type=import_scraper,
                                help="The scraper module to run")
    # TODO: Need the downloader to first save a local metadata file
    #       with the task and the output files

    args = parser.parse_args()

    command_actions = {'validate': validate_config,
                       'dispatch': run_dispatch,
                       'download': run_downloader,
                       'extract': None,
                       }
    command_actions[args.command](args)
